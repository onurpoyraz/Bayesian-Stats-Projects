{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onur Poyraz 2016705069"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hereby declare that I observed the honour code of the university when preparing the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pr?gr?mm?ng?H?m?w?rk 3\n",
    "\n",
    "In this exercise we model a string of text using a Markov(1) model. For simplicity we only consider letters 'a-z'. Capital letters 'A-Z' are mapped to the corresponding ones. All remaining letters, symbols, numbers, including spaces, are denoted by '.'.\n",
    "\n",
    "\n",
    "We have a probability table $T$ where $T_{i,j} = p(x_t = j | x_{t-1} = i)$  transition model of letters in English text for $t=1,2 \\dots N$. Assume that the initial letter in a string is always a space denoted as $x_0 = \\text{'.'}$. Such a model where the probability table is always the same is sometimes called a stationary model.\n",
    "\n",
    "1. For a given $N$, write a program to sample random strings with letters $x_1, x_2, \\dots, x_N$ from $p(x_{1:N}|x_0)$\n",
    "1. Now suppose you are given strings with missing letters, where each missing letter is denoted by a question mark (or underscore, as below). Implement a method, that samples missing letters conditioned on observed ones, i.e., samples from $p(x_{-\\alpha}|x_{\\alpha})$ where $\\alpha$ denotes indices of observed letters. For example, if the input is 't??.', we have $N=4$ and\n",
    "$x_1 = \\text{'t'}$ and $x_4 = \\text{'.'}$, $\\alpha=\\{1,4\\}$ and $-\\alpha=\\{2,3\\}$. Your program may possibly generate the strings 'the.', 'twi.', 'tee.', etc. Hint: make sure to make use all data given and sample from the correct distribution. Implement the method and print the results for the test strings below. \n",
    "1. Describe a method for filling in the gaps by estimating the most likely letter for each position. Hint: you need to compute\n",
    "$$\n",
    "x_{-\\alpha}^* = \\arg\\max_{x_{-\\alpha}} p(x_{-\\alpha}|x_{\\alpha})\n",
    "$$\n",
    "Implement the method and print the results for the following test strings along with the log-probability  $\\log p(x_{-\\alpha}^*,x_{\\alpha})$.\n",
    "1. Discuss how you can improve the model to get better estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_strings = ['th__br__n.f_x.', '_u_st__n_.to_be._nsw_r__','i__at_._a_h_n_._e_r_i_g','q___t.___z._____t.__.___.__.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: The code below loads a table of transition probabilities for English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9949749\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t | x_{t-1} = \\text{'a'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0.0002835\n",
      "b 0.0228302\n",
      "c 0.0369041\n",
      "d 0.0426290\n",
      "e 0.0012216\n",
      "f 0.0075739\n",
      "g 0.0171385\n",
      "h 0.0014659\n",
      "i 0.0372661\n",
      "j 0.0002353\n",
      "k 0.0110124\n",
      "l 0.0778259\n",
      "m 0.0260757\n",
      "n 0.2145354\n",
      "o 0.0005459\n",
      "p 0.0195213\n",
      "q 0.0001749\n",
      "r 0.1104770\n",
      "s 0.0934290\n",
      "t 0.1317960\n",
      "u 0.0098029\n",
      "v 0.0306574\n",
      "w 0.0088799\n",
      "x 0.0009562\n",
      "y 0.0233701\n",
      "z 0.0018701\n",
      ". 0.0715219\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from IPython.display import display, Latex, Math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "alphabet = [chr(i+ord('a')) for i in range(26)]\n",
    "alphabet.append('.')\n",
    "letter2idx = {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "T = []\n",
    "with open('transitions.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        T.append(row)\n",
    "\n",
    "print('Example')\n",
    "## p(x_t = 'u' | x_{t-1} = 'q')\n",
    "display(Latex(r\"$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$\"))\n",
    "print (T[letter2idx['q']][letter2idx['u']])\n",
    "display(Latex(r\"$p(x_t | x_{t-1} = \\text{'a'})$\"))\n",
    "for c,p in zip(alphabet,T[letter2idx['a']]):\n",
    "    print(c,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly I took T lists as a matrix Data and I normalize the data because it is not completely normalised. (I see that some of the rows sum is not equal to 1 exactly. Some of them is equal to 1.01 and some of them are equals to 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.062437e-01 4.445020e-02 3.916000e-02 2.829470e-02 2.130840e-02\n",
      " 4.007930e-02 1.717830e-02 6.060470e-02 6.781650e-02 3.466000e-03\n",
      " 4.545100e-03 2.430190e-02 4.064290e-02 2.348820e-02 6.499200e-02\n",
      " 2.734980e-02 2.220800e-03 2.140680e-02 7.046870e-02 1.460781e-01\n",
      " 9.239900e-03 7.949700e-03 6.063850e-02 1.107000e-04 1.146380e-02\n",
      " 2.911000e-04 5.621020e-02]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "Data_load = np.loadtxt(\"transitions.txt\",dtype=float)\n",
    "Data = normalize(Data_load, axis=1, norm='l1')\n",
    "print (Data[26,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part I randomly choose letters over probablity distributions according to previous letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".it.in.istanghisthom.s.m.y.ig.tr.het.inc.st.s.hthemesan.thenoun.ds.hen..hesindabllore.emasheny.grount\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "text='.'\n",
    "\n",
    "for k in range(N):\n",
    "    index=alphabet.index(text[k])\n",
    "    output = np.random.choice(alphabet,1, p=Data[index,:])\n",
    "    text=text+output[0]\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am try to estimate the missing letters according to first previous and first later non-missing letter. I am randomly choose missing letters over probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".the.brdon.fex..\n",
      ".futstheng.toobe.insweror.\n",
      ".ioeath.pathans.ceerting.\n",
      ".qusat.yerz..e.int.he.agd.io..\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_strings)):\n",
    "    test = '.' + test_strings[i] + '.'\n",
    "    text = list(test)\n",
    "    k = 0\n",
    "    for j in range(k,len(text)):\n",
    "        if text[j]=='_':\n",
    "            first_letter = text[j-1]\n",
    "            first_index=alphabet.index(first_letter)\n",
    "            count = 0\n",
    "            for k in range(j+1,len(text)):\n",
    "                if text[k] == '_':\n",
    "                    count = count + 1\n",
    "                else:\n",
    "                    last_letter = text[k]\n",
    "                    last_index = alphabet.index(last_letter)\n",
    "                    break\n",
    "            for l in range(j,k):\n",
    "                first_element = np.dot(Data[first_index,:],np.linalg.matrix_power(Data,l+count+1-k))\n",
    "                second_element = np.dot(np.linalg.matrix_power(Data,k-l-1),Data[:,last_index])\n",
    "                probability = np.multiply(first_element,second_element.T)/np.dot(first_element,second_element)\n",
    "                text[l] = np.random.choice(alphabet,1, p=probability)[0]\n",
    "    print (\"\".join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part I made almost everyhing same as previous part. I just did not pick missing letters randomly. I pick letters that maximize the probability over probability table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".the.br.an.fex..\n",
      ".oursthand.to.be.answeres.\n",
      ".in.ath.wathend.he.r.ing.\n",
      ".qur.t.t.rz.t....t.ae.t.e.ae..\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_strings)):\n",
    "    test = '.' + test_strings[i] + '.'\n",
    "    text = list(test)\n",
    "    for j in range(len(text)):\n",
    "        if text[j]=='_':\n",
    "            first_letter = text[j-1]\n",
    "            first_index=alphabet.index(first_letter)\n",
    "            count = 0\n",
    "            for k in range(j+1,len(text)):\n",
    "                if text[k] == '_':\n",
    "                    count = count + 1\n",
    "                else:\n",
    "                    last_letter = text[k]\n",
    "                    last_index = alphabet.index(last_letter)\n",
    "                    break\n",
    "            for l in range(j,k):\n",
    "                first_element = np.dot(Data[first_index,:],np.linalg.matrix_power(Data,l+count+1-k))\n",
    "                second_element = np.dot(np.linalg.matrix_power(Data,k-l-1),Data[:,last_index])\n",
    "                probability = np.multiply(first_element,second_element.T)/np.dot(first_element,second_element)\n",
    "                choise = np.argmax(probability)\n",
    "                text[l] = alphabet[choise]\n",
    "    print (\"\".join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use such systems that calculate more than one letter before missing letter and also more than one letter after missing letter. Also we have to analyze lower case and upper case letters as well. It is of course hard to programming but will be much more efficient. One another problem with this data is that sometimes data can stuck at some part of it. For example for last test string there is 4 '.' because when first letter equals to '.' and last letter equals to 't' it stucks at that point and generate all letter as '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
